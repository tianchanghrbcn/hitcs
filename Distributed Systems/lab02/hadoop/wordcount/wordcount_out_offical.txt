PS D:\ComputerScience\hitcs\hitcs\Distributed Systems\lab02\hadoop> # 1) 准备临时输入目录 & 小文本
PS D:\ComputerScience\hitcs\hitcs\Distributed Systems\lab02\hadoop> $rnd = Get-Random
PS D:\ComputerScience\hitcs\hitcs\Distributed Systems\lab02\hadoop> $IN  = "/user/test/wordcount_in_$rnd"
PS D:\ComputerScience\hitcs\hitcs\Distributed Systems\lab02\hadoop> $OUT = "/user/test/wordcount_out_$rnd"
PS D:\ComputerScience\hitcs\hitcs\Distributed Systems\lab02\hadoop>
PS D:\ComputerScience\hitcs\hitcs\Distributed Systems\lab02\hadoop> #   用这里的 Here-String 写入一段示例文字
PS D:\ComputerScience\hitcs\hitcs\Distributed Systems\lab02\hadoop> $text = @"
>> An Apache Hadoop MapReduce job works by breaking the processing
>> into two phases: map and reduce. The map phase processes data in parallel.
>> "@
PS D:\ComputerScience\hitcs\hitcs\Distributed Systems\lab02\hadoop> #   写入本地临时文件
PS D:\ComputerScience\hitcs\hitcs\Distributed Systems\lab02\hadoop> $tmpFile = "$PWD\sample.txt"
PS D:\ComputerScience\hitcs\hitcs\Distributed Systems\lab02\hadoop> $text | Set-Content -Encoding UTF8 $tmpFile
PS D:\ComputerScience\hitcs\hitcs\Distributed Systems\lab02\hadoop>
PS D:\ComputerScience\hitcs\hitcs\Distributed Systems\lab02\hadoop> # 2) 上传到 HDFS
PS D:\ComputerScience\hitcs\hitcs\Distributed Systems\lab02\hadoop> hdfs dfs -mkdir -p $IN
PS D:\ComputerScience\hitcs\hitcs\Distributed Systems\lab02\hadoop> hdfs dfs -put -f $tmpFile $IN/
PS D:\ComputerScience\hitcs\hitcs\Distributed Systems\lab02\hadoop>
PS D:\ComputerScience\hitcs\hitcs\Distributed Systems\lab02\hadoop> # 3) 运行官方 WordCount
PS D:\ComputerScience\hitcs\hitcs\Distributed Systems\lab02\hadoop> $EX = Get-ChildItem "$env:HADOOP_HOME\share\hadoop\mapreduce" `
>>       -Filter 'hadoop-mapreduce-examples-*.jar' | Select-Object -First 1
PS D:\ComputerScience\hitcs\hitcs\Distributed Systems\lab02\hadoop> hadoop jar $EX wordcount $IN $OUT
2025-05-28 20:27:09,777 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
2025-05-28 20:27:09,801 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2025-05-28 20:27:09,801 INFO impl.MetricsSystemImpl: JobTracker metrics system started
2025-05-28 20:27:10,147 INFO input.FileInputFormat: Total input files to process : 1
2025-05-28 20:27:10,161 INFO mapreduce.JobSubmitter: number of splits:1
2025-05-28 20:27:10,205 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local836648492_0001
2025-05-28 20:27:10,205 INFO mapreduce.JobSubmitter: Executing with tokens: []
2025-05-28 20:27:10,271 INFO mapreduce.Job: The url to track the job: http://localhost:8080/
2025-05-28 20:27:10,271 INFO mapreduce.Job: Running job: job_local836648492_0001
2025-05-28 20:27:10,272 INFO mapred.LocalJobRunner: OutputCommitter set in config null
2025-05-28 20:27:10,278 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2
2025-05-28 20:27:10,278 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2025-05-28 20:27:10,279 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2025-05-28 20:27:10,321 INFO mapred.LocalJobRunner: Waiting for map tasks
2025-05-28 20:27:10,325 INFO mapred.LocalJobRunner: Starting task: attempt_local836648492_0001_m_000000_0
2025-05-28 20:27:10,336 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2
2025-05-28 20:27:10,336 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2025-05-28 20:27:10,341 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
2025-05-28 20:27:10,362 INFO mapred.Task:  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@388fea06
2025-05-28 20:27:10,374 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/test/wordcount_in_453863523/sample.txt:0+140
2025-05-28 20:27:10,401 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
2025-05-28 20:27:10,402 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100
2025-05-28 20:27:10,404 INFO mapred.MapTask: soft limit at 83886080
2025-05-28 20:27:10,404 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600
2025-05-28 20:27:10,405 INFO mapred.MapTask: kvstart = 26214396; length = 6553600
2025-05-28 20:27:10,407 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2025-05-28 20:27:10,651 INFO mapred.LocalJobRunner:
2025-05-28 20:27:10,652 INFO mapred.MapTask: Starting flush of map output
2025-05-28 20:27:10,654 INFO mapred.MapTask: Spilling map output
2025-05-28 20:27:10,654 INFO mapred.MapTask: bufstart = 0; bufend = 231; bufvoid = 104857600
2025-05-28 20:27:10,654 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214308(104857232); length = 89/6553600
2025-05-28 20:27:10,662 INFO mapred.MapTask: Finished spill 0
2025-05-28 20:27:10,669 INFO mapred.Task: Task:attempt_local836648492_0001_m_000000_0 is done. And is in the process of committing
2025-05-28 20:27:10,671 INFO mapred.LocalJobRunner: map
2025-05-28 20:27:10,671 INFO mapred.Task: Task 'attempt_local836648492_0001_m_000000_0' done.
2025-05-28 20:27:10,675 INFO mapred.Task: Final Counters for attempt_local836648492_0001_m_000000_0: Counters: 24
        File System Counters
                FILE: Number of bytes read=281174
                FILE: Number of bytes written=913902
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=140
                HDFS: Number of bytes written=0
                HDFS: Number of read operations=5
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=1
                HDFS: Number of bytes read erasure-coded=0
        Map-Reduce Framework
                Map input records=2
                Map output records=23
                Map output bytes=231
                Map output materialized bytes=273
                Input split bytes=130
                Combine input records=23
                Combine output records=22
                Spilled Records=22
                Failed Shuffles=0
                Merged Map outputs=0
                GC time elapsed (ms)=4
                Total committed heap usage (bytes)=357564416
        File Input Format Counters
                Bytes Read=140
2025-05-28 20:27:10,676 INFO mapred.LocalJobRunner: Finishing task: attempt_local836648492_0001_m_000000_0
2025-05-28 20:27:10,677 INFO mapred.LocalJobRunner: map task executor complete.
2025-05-28 20:27:10,678 INFO mapred.LocalJobRunner: Waiting for reduce tasks
2025-05-28 20:27:10,678 INFO mapred.LocalJobRunner: Starting task: attempt_local836648492_0001_r_000000_0
2025-05-28 20:27:10,682 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2
2025-05-28 20:27:10,691 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2025-05-28 20:27:10,692 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
2025-05-28 20:27:10,708 INFO mapred.Task:  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@2d9c1329
2025-05-28 20:27:10,710 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@3cb2c0a6
2025-05-28 20:27:10,711 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2025-05-28 20:27:10,720 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10
2025-05-28 20:27:10,721 INFO reduce.EventFetcher: attempt_local836648492_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
2025-05-28 20:27:10,735 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local836648492_0001_m_000000_0 decomp: 269 len: 273 to MEMORY
2025-05-28 20:27:10,746 INFO reduce.InMemoryMapOutput: Read 269 bytes from map-output for attempt_local836648492_0001_m_000000_0
2025-05-28 20:27:10,747 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 269, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->269
2025-05-28 20:27:10,748 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning
2025-05-28 20:27:10,749 INFO mapred.LocalJobRunner: 1 / 1 copied.
2025-05-28 20:27:10,749 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
2025-05-28 20:27:10,754 INFO mapred.Merger: Merging 1 sorted segments
2025-05-28 20:27:10,755 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 264 bytes
2025-05-28 20:27:10,756 INFO reduce.MergeManagerImpl: Merged 1 segments, 269 bytes to disk to satisfy reduce memory limit
2025-05-28 20:27:10,756 INFO reduce.MergeManagerImpl: Merging 1 files, 273 bytes from disk
2025-05-28 20:27:10,757 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce
2025-05-28 20:27:10,757 INFO mapred.Merger: Merging 1 sorted segments
2025-05-28 20:27:10,758 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 264 bytes
2025-05-28 20:27:10,758 INFO mapred.LocalJobRunner: 1 / 1 copied.
2025-05-28 20:27:10,773 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
2025-05-28 20:27:10,807 INFO mapred.Task: Task:attempt_local836648492_0001_r_000000_0 is done. And is in the process of committing
2025-05-28 20:27:10,809 INFO mapred.LocalJobRunner: 1 / 1 copied.
2025-05-28 20:27:10,811 INFO mapred.Task: Task attempt_local836648492_0001_r_000000_0 is allowed to commit now
2025-05-28 20:27:10,817 INFO output.FileOutputCommitter: Saved output of task 'attempt_local836648492_0001_r_000000_0' to hdfs://localhost:9000/user/test/wordcount_out_453863523
2025-05-28 20:27:10,817 INFO mapred.LocalJobRunner: reduce > reduce
2025-05-28 20:27:10,817 INFO mapred.Task: Task 'attempt_local836648492_0001_r_000000_0' done.
2025-05-28 20:27:10,818 INFO mapred.Task: Final Counters for attempt_local836648492_0001_r_000000_0: Counters: 30
        File System Counters
                FILE: Number of bytes read=281752
                FILE: Number of bytes written=914175
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=140
                HDFS: Number of bytes written=179
                HDFS: Number of read operations=10
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=3
                HDFS: Number of bytes read erasure-coded=0
        Map-Reduce Framework
                Combine input records=0
                Combine output records=0
                Reduce input groups=22
                Reduce shuffle bytes=273
                Reduce input records=22
                Reduce output records=22
                Spilled Records=22
                Shuffled Maps =1
                Failed Shuffles=0
                Merged Map outputs=1
                GC time elapsed (ms)=0
                Total committed heap usage (bytes)=357564416
        Shuffle Errors
                BAD_ID=0
                CONNECTION=0
                IO_ERROR=0
                WRONG_LENGTH=0
                WRONG_MAP=0
                WRONG_REDUCE=0
        File Output Format Counters
                Bytes Written=179
2025-05-28 20:27:10,820 INFO mapred.LocalJobRunner: Finishing task: attempt_local836648492_0001_r_000000_0
2025-05-28 20:27:10,820 INFO mapred.LocalJobRunner: reduce task executor complete.
2025-05-28 20:27:11,290 INFO mapreduce.Job: Job job_local836648492_0001 running in uber mode : false
2025-05-28 20:27:11,292 INFO mapreduce.Job:  map 100% reduce 100%
2025-05-28 20:27:11,299 INFO mapreduce.Job: Job job_local836648492_0001 completed successfully
2025-05-28 20:27:11,308 INFO mapreduce.Job: Counters: 36
        File System Counters
                FILE: Number of bytes read=562926
                FILE: Number of bytes written=1828077
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=280
                HDFS: Number of bytes written=179
                HDFS: Number of read operations=15
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=4
                HDFS: Number of bytes read erasure-coded=0
        Map-Reduce Framework
                Map input records=2
                Map output records=23
                Map output bytes=231
                Map output materialized bytes=273
                Input split bytes=130
                Combine input records=23
                Combine output records=22
                Reduce input groups=22
                Reduce shuffle bytes=273
                Reduce input records=22
                Reduce output records=22
                Spilled Records=44
                Shuffled Maps =1
                Failed Shuffles=0
                Merged Map outputs=1
                GC time elapsed (ms)=4
                Total committed heap usage (bytes)=715128832
        Shuffle Errors
                BAD_ID=0
                CONNECTION=0
                IO_ERROR=0
                WRONG_LENGTH=0
                WRONG_MAP=0
                WRONG_REDUCE=0
        File Input Format Counters
                Bytes Read=140
        File Output Format Counters
                Bytes Written=179
PS D:\ComputerScience\hitcs\hitcs\Distributed Systems\lab02\hadoop>
PS D:\ComputerScience\hitcs\hitcs\Distributed Systems\lab02\hadoop> # 4) 查看结果
PS D:\ComputerScience\hitcs\hitcs\Distributed Systems\lab02\hadoop> hdfs dfs -cat "$OUT/part-r-00000"
An      1
Apache  1
Hadoop  1
MapReduce       1
The     1
and     1
breaking        1
by      1
data    1
in      1
into    1
job     1
map     2
parallel.       1
phase   1
phases: 1
processes       1
processing      1
reduce. 1
the     1
two     1
works   1
PS D:\ComputerScience\hitcs\hitcs\Distributed Systems\lab02\hadoop>